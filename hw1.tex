%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2013 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2013,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
% \usepackage{subfigure}
\usepackage{subcaption}
\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% For math
\usepackage{amsmath}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2013}
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
% \usepackage[accepted]{icml2013}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{6.867: Homework 1}

\begin{document}

\twocolumn[
\icmltitle{6.867: Homework 1}

% % It is OKAY to include author information, even for blind
% % submissions: the style file will automatically remove it for you
% % unless you've provided the [accepted] option to the icml2013
% % package.
% \icmlauthor{Your Name}{email@yourdomain.edu}
% \icmladdress{Your Fantastic Institute,
%             314159 Pi St., Palo Alto, CA 94306 USA}
% \icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
% \icmladdress{Their Fantastic Institute,
%             27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{boring formatting information, machine learning, ICML}

\vskip 0.3in
]

\section{Gradient descent}
\label{submission}

As in the past few years, ICML will rely exclusively on
electronic formats for submission and review.


\subsection{Templates for Papers}


\section{Linear basis function regression}

We consider linear basis function regression as a method to benchmark the robustness of the gradient descent solution presented above. By using the closed-form maximum likelihood equation, we can calculate the maximum likelihood weight vector for our list of basis functions to approximate the data in the form of our basis. In this scenario, we are using data generated by $y(x) = \cos(\pi x) + 1.5 \cos(2 \pi x) + \epsilon(x)$, where $\epsilon(x)$ is some added noise to the dataset. Running linear regression on a simple polynomial basis of order $M$, where $\phi_0(x) = x^0$, $\phi_1(x) = x^1$, $\phi_2(x) = x^2$, ..., $\phi_M(x) = x^M$, we calculate the maximum likelihood weight vector by the following:
$$w_{ML} = (\Phi^T \Phi)^{-1} \Phi^T y$$
where $w_{ML}$ is the maximum likelihood weight vector and $\Phi$ is given by:
$$\Phi =
\begin{bmatrix}
  \phi_0(x_0)   & \phi_1(x_0)   & \phi_2(x_0)   & \dots   & \phi_M(x_0) \\
  \phi_0(x_1)   & \phi_1(x_1)   & \phi_2(x_1)   & \dots   & \phi_M(x_1) \\
  \vdots        & \vdots        & \vdots        & \ddots  & \vdots \\
  \phi_0(x_n)   & \phi_1(x_n)   & \phi_2(x_n)   & \dots   & \phi_M(x_n) \\
\end{bmatrix}
$$
Our choice of $M$, the degree of our polynomial basis, largely determines the fit of the regression to the data (Figure 2). More specifically, as small values of $M$, the polynomial basis cannot adequately capture all the data points. At higher values of $M$ however, overfitting occurs in which the weight vector performs well on the training data, but is not well generalized to new data. The value of $M$ therefore must be carefully considered in order to prevent too high variability in our generated regression polynomial. \\
\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P2/linear_regression,0.png}
  \includegraphics[width=1.2\linewidth]{code/P2/linear_regression,3.png}
  \includegraphics[width=1.2\linewidth]{code/P2/linear_regression,1.png}
  \includegraphics[width=1.2\linewidth]{code/P2/linear_regression,10.png}
\end{multicols}
\caption{Linear regression for varying values of M.}
\end{figure}

We can also instead choose our set of basis functions to be the set of cosine functions, where $\phi_1(x) = \cos(\pi x)$, $\phi_2(x) = \cos(2 \pi x)$, ..., $\phi_M(x) = \cos(M \pi x)$. This is again calculated for multiple values of $M$ and compared in Figure 3. Interestingly, even when we use the same family of basis functions as used to generate the initial data ($M=2$), due to the noise $\epsilon(x)$ added to the initial dataset, the maximum likelihood weight vector does not identically match the actual function used:
$$ w =
\begin{bmatrix}
  1 \\
  1.5
\end{bmatrix}$$
$$ w_{MLE} =
\begin{bmatrix}
  0.779 \\
  1.174
\end{bmatrix}$$
where $w$ is the actual weight vector and $w_{MLE}$ is the maximum likelihood estimated weight vector.

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P2/cosine_regression,1.png}
  \includegraphics[width=1.2\linewidth]{code/P2/cosine_regression,3.png}
  \includegraphics[width=1.2\linewidth]{code/P2/cosine_regression,2.png}
  \includegraphics[width=1.2\linewidth]{code/P2/cosine_regression,8.png}
\end{multicols}
\caption{Linear regression for varying values of M.}
\end{figure}
\vspace{3mm}

\section{Ridge regression}

As illustrated in the previous section, some values of $M$ can result in overfitting to the training set. Thus, in an attempt to reduce overfitting, we implement ridge regression, which adds a regularization parameter $\lambda$ to the regression function. In this case, we have chosen a regularizer to serve as a weight decay, in order to encourage weight vector values to tend towards zero unless otherwise supported by the data. For any given value of $M$ therefore, with greater weight decay regularization coefficients, the weight vector decays more strongly towards zero. In other words, for higher values of $\lambda$, the data must more strongly support greater weight vector values in order to acehive the same magnitude of coefficients in the weight vector. \\ \\
This is illustrated in Figure 4, where the first row illustrates the fit of ridge regression using a polynomial basis of degree 3 with $\lambda = 0.1$ and $1$ and the second row shows the fit of the ridge regression using a polynomial basis of degree 10. As illustrated, the weight vector coefficients in the regression are dampened towards zero with greater values of the regularization coefficient. This is compared with the regression computed with a $\lambda = 0$ to illustrate the decay effect of the regulaization parameter.

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P3/ridge_regression,3,01.png}
  \includegraphics[width=1.2\linewidth]{code/P3/ridge_regression,10,01.png}
  \includegraphics[width=1.2\linewidth]{code/P3/ridge_regression,3,1.png}
  \includegraphics[width=1.2\linewidth]{code/P3/ridge_regression,10,1.png}
\end{multicols}
\caption{Ridge regression for values of M and $\lambda$.}
\end{figure}

Using additional data sets for validation and testing, we can select for the best $M$ and $\lambda$ parameters in our regression model using a polynomial basis. More specifically, for many combinations of values of $M$ and $\lambda$, the maximum likelihood weight vector $w_{MLE}$ was calculated based on the training data set. Through model selection on the validation data set, the best model and optimal values for $M$ and $\lambda$ were chosen based on the evaluation metric. This optimal regression model was then run on the testing data set to evaluate the performance of the model on the new data. The maximum likelihood weight vectors were generated for all $M \in [0, 10]$ and $\lambda \in \{0, 0.1, 0.2, ..., 1.4, 1.5\}$, and this was performed multiple times with data set A as the training data and data set B as the testing data, as well as vice versa. The performance of the regression model was evaluated using the sum of squared errors (SSE) and the mean squared errors (MSE); qualitatively, both evaluation metrics result in the same conclusions, although quantitatively, some additional information can be gleaned from each evaluation metric. \\ \\
When data set A was used as the training data and data set B was used as the testing data, the validation step indicated that $M = 2$ and $\lambda=0.0$ yielded the optimal regression model with a minimum SSE of $2.35$ and a minimum MSE of $0.11$. Running this model on the testing data demonstrated that the regression fit the testing data remarkably well, excepting a single outlier in the testing data. Thus, despite a relatively low MSE of $2.58$ on the testing data, the SSE was $25.75$ due to the significant error in the outlier data point. The training, validation, and testing steps are illustrated in Figure 6, in which the best regression line chosen in model selection is plotted against each data set used for training, validation, and testing, respectively.

\begin{figure}[width=\linewidth]
\centering
\begin{multicols}{2}
  \includegraphics[width=1.2\linewidth]{code/P3/training,training_a,testing_b.png}
  \includegraphics[width=1.2\linewidth]{code/P3/validation,training_a,testing_b.png}
  \includegraphics[width=1.2\linewidth]{code/P3/testing,training_a,testing_b.png}
\end{multicols}
\caption{Ridge regression for values of M and $\lambda$.}
\end{figure}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
